{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChentianJiang/miniconda3/envs/handset-model/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# file\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "import handset_model_current as handset_model\n",
    "os.chdir(\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_arg(string):\n",
    "    value = string.lower()\n",
    "    if value == 'true':\n",
    "        return True\n",
    "    elif value == 'false':\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Expected True or False, but got {}\".format(string))\n",
    "\n",
    "# minimal preprocessing\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model hyperparameters\n",
    "# small number of epochs for experimentation\n",
    "parser.add_argument('--epochs', default=10, type=int,\n",
    "                    help=\"Nr of epochs. Default is 100\", dest=\"epochs\")\n",
    "parser.add_argument('--batch_size', default=256, type=int,\n",
    "                    help=\"Batch size. Default is 32\", dest=\"batch_size\")\n",
    "parser.add_argument('--earlystop', default=3, type=int,\n",
    "                    help=\"Number of epochs with no improvement after which training will be stopped.\",\n",
    "                    dest=\"earlystop\")\n",
    "parser.add_argument('--verbose', default=True, type=bool_arg, help=\"If True (default), verbose output\",\n",
    "                    dest=\"verbose\")\n",
    "\n",
    "# cross_val is not ready to be used\n",
    "parser.add_argument('--cross_val', default=0, type=int,\n",
    "                    help=\"Number of folds (if bigger than 0) to use for cross validation. Default is 0.\",\n",
    "                    dest=\"cross_val\")\n",
    "\n",
    "# no applying class weights\n",
    "parser.add_argument('--apply_class_weights', default=False, type=bool_arg,\n",
    "                    help=\"If True, apply different loss weights (based on frequency of samples) to different \"\n",
    "                         \"classes.\",\n",
    "                    dest=\"apply_class_weights\")\n",
    "\n",
    "# no smooth factor\n",
    "parser.add_argument('--smooth_factor', default=0, type=float,\n",
    "                    help=\"Smooth factor to be used when calculating class weights, so that highly unfrequent \"\n",
    "                    \"classes do not get huge weights.\",\n",
    "                    dest=\"smooth_factor\")\n",
    "\n",
    "# oversampling with neg to pos ratio=3\n",
    "parser.add_argument('--oversample', default=True, type=bool_arg,\n",
    "                    help=\"If True (default), apply oversampling to generate balanced batches.\",\n",
    "                    dest=\"oversample\")\n",
    "parser.add_argument('--ratio', default=3, type=int,\n",
    "                    help=\"Ratio of negative to positive samples to use for balanced batch generation \"\n",
    "                         \"(if oversample=True)\",\n",
    "                    dest=\"ratio\")\n",
    "\n",
    "# activation: prelu\n",
    "parser.add_argument('--activation', default='prelu',\n",
    "                    help=\"NN activation to be used. Default is prelu\",\n",
    "                    dest=\"activation\")\n",
    "\n",
    "# no x_vars\n",
    "parser.add_argument('--x_vars', default=False, type=bool_arg, help=\"If True (default), include X variables\",\n",
    "                    dest=\"x_vars\")\n",
    "\n",
    "# standardize numerical data\n",
    "parser.add_argument('--std', default=True, type=bool_arg, help=\"If True (default), standardize data.\",\n",
    "                    dest=\"std\")\n",
    "\n",
    "# no pca\n",
    "parser.add_argument('--pca_whiten', default=False, type=bool_arg, help=\"If True (default), PCA-whiten data.\",\n",
    "                    dest=\"pca_whiten\")\n",
    "parser.add_argument('--pca_reduce', default=0, type=float,\n",
    "                    help=\"{0, 1, 0<x<1} If 0, no dimensionality reduction is done. If 1, Thomas P. Minka's method \"\n",
    "                         \"('Automatic Choice of Dimensionality for PCA'. NIPS 2000) is used to determine the \"\n",
    "                         \"number of dimensions to keep. If 0 < pca_reduce < 1, enough number of dimensions will \"\n",
    "                         \"be kept to keep 'pca_reduce' percentage of variance explained. Default is 0.9.\",\n",
    "                    dest=\"pca_reduce\")\n",
    "\n",
    "# one-hot encode cat data (embeddings are not used)\n",
    "parser.add_argument('--cat_enc', default='one-hot',\n",
    "                    help=\"Encoding to be used for categorical variables. Default is 'integer' \"\n",
    "                         \"(embedding layers will then be used). Other alternatives: 'hashing_char', \"\n",
    "                         \"'hashing_all', 'one-hot'.\",\n",
    "                    dest=\"cat_enc\")\n",
    "\n",
    "# no log transform\n",
    "parser.add_argument('--log_xform', default=False, type=bool_arg, help=\"If True (default), log-transform data.\",\n",
    "                    dest=\"log_xform\")\n",
    "\n",
    "# encode categorical and binary data as 1/0\n",
    "parser.add_argument('--binary_enc', default=True, type=bool_arg,\n",
    "                    help=\"If False (default), the negative cases of binary variables will be represented as -1 \"\n",
    "                         \"instead of 0.\", dest=\"binary_enc\")\n",
    "\n",
    "# id for saving/ loading\n",
    "parser.add_argument('--data_split_id', default=2, type=int,\n",
    "                    help=\"Id for the train-test data split to be used. If a new id is given, a new data split \"\n",
    "                         \"will be generated and saved to disk with the given id. If id is 0 (default), a new \"\n",
    "                         \"split will be generated, but not saved to disk. If a previously used id is given, \"\n",
    "                         \"a previously generated and saved data split with that id will be used.\",\n",
    "                    dest=\"data_split_id\")\n",
    "parser.add_argument(\"-f\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary with levels of catagorical variables...\n",
      "Reusing data split with id=2\n",
      "Loading previously pre-processed numerical data...\n",
      "Loading previously pre-processed categorical data...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "data_train, data_test, cat_levels = handset_model.load_and_preprocess_data(args)  # split_id=2 for f_classif features\n",
    "os.chdir(\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((466632, 7), (466632, 235))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['num'].shape, data_train['cat'].shape  # 2nd dim should be 7 and 235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((466632, 242), (466632,), (116659, 242), (116659,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat([data_train['num'], data_train['cat']], axis=1)\n",
    "y_train = data_train['labels']['TARGET_S_TO_S_APPLE']\n",
    "X_test = pd.concat([data_test['num'], data_test['cat']], axis=1)\n",
    "y_test = data_test['labels']['TARGET_S_TO_S_APPLE']\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 30,\n",
       " 'nthread': -1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 199.61564918314704,\n",
       " 'seed': 0,\n",
       " 'silent': False,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# References:\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/how_to/param_tuning.md\n",
    "\n",
    "npos = len([i for i in y_train if i == 1]) \n",
    "nneg = len([i for i in y_train if i == 0])\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate=0.1,\n",
    "                     n_estimators=30,\n",
    "                     max_depth=5,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective='binary:logistic',\n",
    "                     seed=0,\n",
    "                     scale_pos_weight=nneg/npos,\n",
    "                     silent=False\n",
    "                    )\n",
    "\n",
    "xgb1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the model...\n",
      "saved model/alg as xgb1.pickle\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "alg = xgb1\n",
    "save=True\n",
    "saveName='xgb1.pickle'\n",
    "load=False\n",
    "loadName='xgb1.pickle'\n",
    "\n",
    "if load:\n",
    "    print(\"loading model/alg from \" + loadName)\n",
    "    alg = pickle.load(open(loadName, 'rb'))\n",
    "    print(alg)\n",
    "else:\n",
    "    # Fit the algorithm on the data\n",
    "    print(\"fitting the model...\")\n",
    "    alg.fit(X_train.values, y_train.values, eval_metric='auc')\n",
    "\n",
    "    if save:\n",
    "        # save the algorithm\n",
    "        pickle.dump(alg, open(saveName, 'wb'))\n",
    "        print(\"saved model/alg as \" + saveName)\n",
    "        \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that the model has predicted some positive labels\n",
    "\n",
    "temp = alg.predict(X_train.values)\n",
    "np.unique(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions...\n",
      "train-auc: 0.790120\teval-auc: 0.748477\n",
      "train-accuracy: 0.754290\teval-accuracy: 0.755064\n",
      "train-precision: 0.016544\teval-precision: 0.014937\n",
      "train-recall: 0.826311\teval-recall: 0.741824\n",
      "train-confusion-matrix:\n",
      " [[350054 114252]\n",
      " [   404   1922]]\n",
      "test-confusion-matrix:\n",
      " [[87654 28424]\n",
      " [  150   431]]\n",
      "train-true-pos-rate: 0.826311\teval-true-pos-rate: 0.741824\n"
     ]
    }
   ],
   "source": [
    "print(\"making predictions...\")\n",
    "train_preds = alg.predict(X_train.values)\n",
    "test_preds = alg.predict(X_test.values)\n",
    "\n",
    "roc_auc_train = roc_auc_score(y_train.values, train_preds)\n",
    "roc_auc_test = roc_auc_score(y_test.values, test_preds)\n",
    "\n",
    "a_train = accuracy_score(y_train.values, np.rint(train_preds))\n",
    "a_test = accuracy_score(y_test.values, np.rint(test_preds))\n",
    "\n",
    "p_train = precision_score(y_train.values, np.rint(train_preds))\n",
    "p_test = precision_score(y_test.values, np.rint(test_preds))\n",
    "\n",
    "r_train = recall_score(y_train.values, np.rint(train_preds))\n",
    "r_test = recall_score(y_test.values, np.rint(test_preds))\n",
    "\n",
    "m_train = confusion_matrix(y_train.values, np.rint(train_preds))\n",
    "m_test = confusion_matrix(y_test.values, np.rint(test_preds))\n",
    "true_pos_rate_train = m_train[1][1]/(m_train[1][1]+m_train[1][0])\n",
    "true_pos_rate_test = m_test[1][1]/(m_test[1][1]+m_test[1][0])\n",
    "\n",
    "print('train-auc: %f\\teval-auc: %f' % (roc_auc_train, roc_auc_test))\n",
    "print('train-accuracy: %f\\teval-accuracy: %f' % (a_train, a_test))\n",
    "print('train-precision: %f\\teval-precision: %f' % (p_train, p_test))\n",
    "print('train-recall: %f\\teval-recall: %f' % (r_train, r_test))\n",
    "\n",
    "print('train-confusion-matrix:\\n', m_train)\n",
    "print('test-confusion-matrix:\\n', m_test)\n",
    "print('train-true-pos-rate: %f\\teval-true-pos-rate: %f' % (true_pos_rate_train, true_pos_rate_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of grid search for param tuning (takes a really long time)\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb1,\n",
    "                        param_grid = param_test1,\n",
    "                        scoring='precision',\n",
    "                        n_jobs=-1,\n",
    "                        iid=False, \n",
    "                        cv=3,\n",
    "                        verbose=10\n",
    "                       )\n",
    "\n",
    "gsearch1.fit(X_train.values, y_train.values)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python3 (handset-model)",
   "language": "python",
   "name": "handset-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
