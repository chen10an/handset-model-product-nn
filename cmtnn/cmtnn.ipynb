{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation, concatenate, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# file\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "import handset_model_current as handset_model\n",
    "os.chdir(\"cmtnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_arg(string):\n",
    "    value = string.lower()\n",
    "    if value == 'true':\n",
    "        return True\n",
    "    elif value == 'false':\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Expected True or False, but got {}\".format(string))\n",
    "\n",
    "# minimal preprocessing\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model hyperparameters\n",
    "# small number of epochs for experimentation\n",
    "parser.add_argument('--epochs', default=10, type=int,\n",
    "                    help=\"Nr of epochs. Default is 100\", dest=\"epochs\")\n",
    "parser.add_argument('--batch_size', default=256, type=int,\n",
    "                    help=\"Batch size. Default is 32\", dest=\"batch_size\")\n",
    "parser.add_argument('--earlystop', default=3, type=int,\n",
    "                    help=\"Number of epochs with no improvement after which training will be stopped.\",\n",
    "                    dest=\"earlystop\")\n",
    "parser.add_argument('--verbose', default=True, type=bool_arg, help=\"If True (default), verbose output\",\n",
    "                    dest=\"verbose\")\n",
    "\n",
    "# cross_val is not ready to be used\n",
    "parser.add_argument('--cross_val', default=0, type=int,\n",
    "                    help=\"Number of folds (if bigger than 0) to use for cross validation. Default is 0.\",\n",
    "                    dest=\"cross_val\")\n",
    "\n",
    "# no applying class weights\n",
    "parser.add_argument('--apply_class_weights', default=False, type=bool_arg,\n",
    "                    help=\"If True, apply different loss weights (based on frequency of samples) to different \"\n",
    "                         \"classes.\",\n",
    "                    dest=\"apply_class_weights\")\n",
    "\n",
    "# no smooth factor\n",
    "parser.add_argument('--smooth_factor', default=0, type=float,\n",
    "                    help=\"Smooth factor to be used when calculating class weights, so that highly unfrequent \"\n",
    "                    \"classes do not get huge weights.\",\n",
    "                    dest=\"smooth_factor\")\n",
    "\n",
    "# oversampling with neg to pos ratio=3\n",
    "parser.add_argument('--oversample', default=True, type=bool_arg,\n",
    "                    help=\"If True (default), apply oversampling to generate balanced batches.\",\n",
    "                    dest=\"oversample\")\n",
    "parser.add_argument('--ratio', default=3, type=int,\n",
    "                    help=\"Ratio of negative to positive samples to use for balanced batch generation \"\n",
    "                         \"(if oversample=True)\",\n",
    "                    dest=\"ratio\")\n",
    "\n",
    "# activation: prelu\n",
    "parser.add_argument('--activation', default='prelu',\n",
    "                    help=\"NN activation to be used. Default is prelu\",\n",
    "                    dest=\"activation\")\n",
    "\n",
    "# no x_vars\n",
    "parser.add_argument('--x_vars', default=False, type=bool_arg, help=\"If True (default), include X variables\",\n",
    "                    dest=\"x_vars\")\n",
    "\n",
    "# standardize numerical data\n",
    "parser.add_argument('--std', default=True, type=bool_arg, help=\"If True (default), standardize data.\",\n",
    "                    dest=\"std\")\n",
    "\n",
    "# no pca\n",
    "parser.add_argument('--pca_whiten', default=False, type=bool_arg, help=\"If True (default), PCA-whiten data.\",\n",
    "                    dest=\"pca_whiten\")\n",
    "parser.add_argument('--pca_reduce', default=0, type=float,\n",
    "                    help=\"{0, 1, 0<x<1} If 0, no dimensionality reduction is done. If 1, Thomas P. Minka's method \"\n",
    "                         \"('Automatic Choice of Dimensionality for PCA'. NIPS 2000) is used to determine the \"\n",
    "                         \"number of dimensions to keep. If 0 < pca_reduce < 1, enough number of dimensions will \"\n",
    "                         \"be kept to keep 'pca_reduce' percentage of variance explained. Default is 0.9.\",\n",
    "                    dest=\"pca_reduce\")\n",
    "\n",
    "# one-hot encode cat data (embeddings are not used)\n",
    "parser.add_argument('--cat_enc', default='one-hot',\n",
    "                    help=\"Encoding to be used for categorical variables. Default is 'integer' \"\n",
    "                         \"(embedding layers will then be used). Other alternatives: 'hashing_char', \"\n",
    "                         \"'hashing_all', 'one-hot'.\",\n",
    "                    dest=\"cat_enc\")\n",
    "\n",
    "# no log transform\n",
    "parser.add_argument('--log_xform', default=False, type=bool_arg, help=\"If True (default), log-transform data.\",\n",
    "                    dest=\"log_xform\")\n",
    "\n",
    "# encode categorical and binary data as 1/0\n",
    "parser.add_argument('--binary_enc', default=True, type=bool_arg,\n",
    "                    help=\"If False (default), the negative cases of binary variables will be represented as -1 \"\n",
    "                         \"instead of 0.\", dest=\"binary_enc\")\n",
    "\n",
    "# id for saving/ loading\n",
    "parser.add_argument('--data_split_id', default=2, type=int,\n",
    "                    help=\"Id for the train-test data split to be used. If a new id is given, a new data split \"\n",
    "                         \"will be generated and saved to disk with the given id. If id is 0 (default), a new \"\n",
    "                         \"split will be generated, but not saved to disk. If a previously used id is given, \"\n",
    "                         \"a previously generated and saved data split with that id will be used.\",\n",
    "                    dest=\"data_split_id\")\n",
    "parser.add_argument(\"-f\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dictionary with levels of catagorical variables...\n",
      "Reusing data split with id=2\n",
      "Loading previously pre-processed numerical data...\n",
      "Loading previously pre-processed categorical data...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "data_train, data_test, cat_levels = handset_model.load_and_preprocess_data(args)  # split_id=2 for f_classif features\n",
    "os.chdir(\"cmtnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((466632, 7), (466632, 235))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['num'].shape, data_train['cat'].shape  # 2nd dim should be 7 and 235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = handset_model.create_model(data_train[\"num\"].columns, data_train[\"cat\"].columns, cat_encoding=args.cat_enc,\n",
    "                     cat_emb_dim=handset_model.CAT_EMB_DIM, cat_levels=cat_levels, include_x_vars=args.x_vars,\n",
    "                     activation=args.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class_weights:  {0: 1.0, 1: 1.0}\n",
      "Oversampling with ration neg/pos= 1\n",
      "Epoch 1/10\n",
      "3624/3627 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8199 - precision: 0.7823 - recall: 0.8863 - fmeasure: 0.8306 - gmean: 0.8324Epoch 00000: val_acc improved from -inf to 0.75637, saving model to handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 52s - loss: 0.3927 - acc: 0.8199 - precision: 0.7823 - recall: 0.8864 - fmeasure: 0.8306 - gmean: 0.8324 - val_loss: 0.4146 - val_acc: 0.7564 - val_precision: 0.0152 - val_recall: 0.5203 - val_fmeasure: 0.0293 - val_gmean: 0.0858\n",
      "Epoch 2/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8536 - precision: 0.8087 - recall: 0.9271 - fmeasure: 0.8636 - gmean: 0.8658Epoch 00001: val_acc improved from 0.75637 to 0.78081, saving model to handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 53s - loss: 0.3312 - acc: 0.8536 - precision: 0.8087 - recall: 0.9271 - fmeasure: 0.8636 - gmean: 0.8658 - val_loss: 0.3712 - val_acc: 0.7808 - val_precision: 0.0157 - val_recall: 0.4812 - val_fmeasure: 0.0301 - val_gmean: 0.0839\n",
      "Epoch 3/10\n",
      "3626/3627 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8639 - precision: 0.8196 - recall: 0.9343 - fmeasure: 0.8729 - gmean: 0.8749Epoch 00002: val_acc did not improve\n",
      "3627/3627 [==============================] - 54s - loss: 0.3122 - acc: 0.8639 - precision: 0.8196 - recall: 0.9343 - fmeasure: 0.8729 - gmean: 0.8749 - val_loss: 0.3881 - val_acc: 0.7718 - val_precision: 0.0153 - val_recall: 0.4913 - val_fmeasure: 0.0295 - val_gmean: 0.0838\n",
      "Epoch 4/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.8702 - precision: 0.8261 - recall: 0.9389 - fmeasure: 0.8786 - gmean: 0.8805Epoch 00003: val_acc improved from 0.78081 to 0.79545, saving model to handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 59s - loss: 0.3006 - acc: 0.8702 - precision: 0.8261 - recall: 0.9389 - fmeasure: 0.8786 - gmean: 0.8805 - val_loss: 0.3496 - val_acc: 0.7954 - val_precision: 0.0161 - val_recall: 0.4627 - val_fmeasure: 0.0307 - val_gmean: 0.0832\n",
      "Epoch 5/10\n",
      "3623/3627 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.8747 - precision: 0.8313 - recall: 0.9413 - fmeasure: 0.8826 - gmean: 0.8844Epoch 00004: val_acc improved from 0.79545 to 0.80211, saving model to handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 58s - loss: 0.2914 - acc: 0.8747 - precision: 0.8313 - recall: 0.9413 - fmeasure: 0.8826 - gmean: 0.8844 - val_loss: 0.3437 - val_acc: 0.8021 - val_precision: 0.0165 - val_recall: 0.4633 - val_fmeasure: 0.0315 - val_gmean: 0.0844\n",
      "Epoch 6/10\n",
      "3624/3627 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.8780 - precision: 0.8345 - recall: 0.9440 - fmeasure: 0.8855 - gmean: 0.8874Epoch 00005: val_acc did not improve\n",
      "3627/3627 [==============================] - 58s - loss: 0.2854 - acc: 0.8780 - precision: 0.8345 - recall: 0.9440 - fmeasure: 0.8856 - gmean: 0.8874 - val_loss: 0.3576 - val_acc: 0.7979 - val_precision: 0.0162 - val_recall: 0.4572 - val_fmeasure: 0.0309 - val_gmean: 0.0829\n",
      "Epoch 7/10\n",
      "3624/3627 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.8807 - precision: 0.8374 - recall: 0.9460 - fmeasure: 0.8881 - gmean: 0.8899Epoch 00006: val_acc did not improve\n",
      "3627/3627 [==============================] - 59s - loss: 0.2805 - acc: 0.8807 - precision: 0.8374 - recall: 0.9460 - fmeasure: 0.8881 - gmean: 0.8899 - val_loss: 0.3687 - val_acc: 0.7895 - val_precision: 0.0159 - val_recall: 0.4731 - val_fmeasure: 0.0305 - val_gmean: 0.0836\n",
      "Epoch 8/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.8827 - precision: 0.8395 - recall: 0.9473 - fmeasure: 0.8899 - gmean: 0.8916Epoch 00007: val_acc improved from 0.80211 to 0.80583, saving model to handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 62s - loss: 0.2763 - acc: 0.8827 - precision: 0.8395 - recall: 0.9473 - fmeasure: 0.8899 - gmean: 0.8916 - val_loss: 0.3357 - val_acc: 0.8058 - val_precision: 0.0170 - val_recall: 0.4683 - val_fmeasure: 0.0324 - val_gmean: 0.0860\n",
      "Epoch 9/10\n",
      "3626/3627 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.8852 - precision: 0.8426 - recall: 0.9484 - fmeasure: 0.8920 - gmean: 0.8938Epoch 00008: val_acc did not improve\n",
      "3627/3627 [==============================] - 56s - loss: 0.2724 - acc: 0.8852 - precision: 0.8426 - recall: 0.9484 - fmeasure: 0.8920 - gmean: 0.8938 - val_loss: 0.3408 - val_acc: 0.8056 - val_precision: 0.0170 - val_recall: 0.4684 - val_fmeasure: 0.0324 - val_gmean: 0.0860\n",
      "Epoch 10/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.8863 - precision: 0.8435 - recall: 0.9497 - fmeasure: 0.8931 - gmean: 0.8949Epoch 00009: val_acc improved from 0.80583 to 0.81775, saving model to handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 56s - loss: 0.2693 - acc: 0.8863 - precision: 0.8435 - recall: 0.9497 - fmeasure: 0.8931 - gmean: 0.8949 - val_loss: 0.3165 - val_acc: 0.8178 - val_precision: 0.0172 - val_recall: 0.4400 - val_fmeasure: 0.0326 - val_gmean: 0.0837\n"
     ]
    }
   ],
   "source": [
    "chkp_file = \"handset_weights.best.hdf5\"\n",
    "handset_model.train_and_evaluate_model(model, data_train, data_test, nb_epochs=args.epochs,\n",
    "                                 batch_size=args.batch_size,\n",
    "                                 oversample=args.oversample,\n",
    "                                 apply_class_weights=args.apply_class_weights,\n",
    "                                 smooth_factor=args.smooth_factor,\n",
    "                                 verbose=args.verbose, chkp_file=chkp_file,\n",
    "                                 earlystop_pat=args.earlystop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = data_train.copy()\n",
    "fdata_test = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdata_train['labels'] = data_train['labels'] ^ 1\n",
    "fdata_test['labels'] = data_test['labels'] ^ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmodel = handset_model.create_model(fdata_train[\"num\"].columns, fdata_train[\"cat\"].columns, cat_encoding=args.cat_enc,\n",
    "                     cat_emb_dim=handset_model.CAT_EMB_DIM, cat_levels=cat_levels, include_x_vars=args.x_vars,\n",
    "                     activation=args.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# one small change to adapt the generator to the falsity model: reverse 0/1\n",
    "class OverSamplingBatchGenerator:\n",
    "    def __init__(self, data_train, batch_size=32, r=1):\n",
    "        # n_batch = n_batch_neg + n_batch_pos\n",
    "        # r = n_batch_neg / n_batch_pos\n",
    "        # N = floor(n_neg / n_batch_neg) = floor(n_neg * (1 + r) / (n_batch * r))\n",
    "\n",
    "       # if (batch_size % (1 + r)) <> 0:\n",
    "       #     raise Exception(\"batch_size must be divisible by (1 + r)\")\n",
    "\n",
    "        self.data_train = data_train\n",
    "        self.neg_idx = (data_train[\"labels\"][data_train[\"labels\"][handset_model.LABEL_COL] == 1]).index.values  # reverse 0 to 1\n",
    "        self.pos_idx = (data_train[\"labels\"][data_train[\"labels\"][handset_model.LABEL_COL] == 0]).index.values  # reverse 1 to 0\n",
    "        # Number of positive and negative examples per batch\n",
    "        self.n_batch_pos = int(batch_size / (1 + r))\n",
    "        self.n_batch_neg = (batch_size - self.n_batch_pos)\n",
    "        # Total number of negative examples\n",
    "        n_neg = self.neg_idx.size\n",
    "        # Number of batches\n",
    "        self.N = int((n_neg * (1 + r) / (batch_size * r)))\n",
    "        if self.N*self.n_batch_neg > n_neg:\n",
    "            self.N = int(n_neg/self.n_batch_neg)\n",
    "\n",
    "    def get_no_batches(self):\n",
    "        return self.N\n",
    "\n",
    "    def generator(self):\n",
    "        labels = np.vstack([np.zeros((self.n_batch_pos, 1)), np.ones((self.n_batch_neg, 1))])  # reverse zeros/ones\n",
    "        while True:\n",
    "            np.random.shuffle(self.neg_idx)\n",
    "            # Shuffle negative data at the beginning of each epoch. There should be self.N steps per epoch\n",
    "            # (i.e., one complete for-loop).\n",
    "            # No need to shuffle positive data, since we randomly sample from it for every batch\n",
    "\n",
    "            for start_idx in range(0, self.N * self.n_batch_neg, self.n_batch_neg):\n",
    "                batch_pos_idx = np.random.choice(self.pos_idx, self.n_batch_pos, replace=False)\n",
    "                batch_neg_idx = self.neg_idx[start_idx: (start_idx + self.n_batch_neg)]\n",
    "                batch = [np.vstack([self.data_train[\"num\"].loc[batch_pos_idx].values,\n",
    "                                    self.data_train[\"num\"].loc[batch_neg_idx].values])] + \\\n",
    "                [np.vstack([self.data_train[\"cat\"].loc[batch_pos_idx].values,\n",
    "                            self.data_train[\"cat\"].loc[batch_neg_idx].values])]\n",
    "                batch.append(labels)\n",
    "                batch = shuffle(*batch)\n",
    "\n",
    "                yield (batch[0:-1], batch[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a local train_and_evaluate_model function that uses the generator above\n",
    "def train_and_evaluate_model(model, data_train, data_test, nb_epochs=100,\n",
    "                             batch_size=32, cvscores=None, verbose=False, chkp_file=None,\n",
    "                             earlystop_pat=10,\n",
    "                             r_balanced_batch=1, oversample = True, apply_class_weights=False,\n",
    "                             smooth_factor=0.1):\n",
    "\n",
    "    # Define callbacks for early stopping and model checkpointing\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=earlystop_pat, verbose=True, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(chkp_file, monitor='val_acc', verbose=verbose, save_best_only=True, mode='max')\n",
    "\n",
    "    class_weights = {0: 1.0, 1:1.0}\n",
    "    if apply_class_weights:\n",
    "            y = data_train[\"labels\"].values\n",
    "            class_weights = get_class_weights(y.reshape((1, y.shape[0]))[0], smooth_factor=smooth_factor)\n",
    "    print(\"Using class_weights: \", class_weights)\n",
    "\n",
    "    # Fit the model\n",
    "    if oversample:\n",
    "        # fixed input and output shapes for generator\n",
    "        print(\"Oversampling with ration neg/pos=\", r_balanced_batch)\n",
    "        gen = OverSamplingBatchGenerator(data_train, batch_size=batch_size, r=r_balanced_batch)\n",
    "\n",
    "        history = model.fit_generator(gen.generator(),\n",
    "                                  validation_data=(\n",
    "                                      [data_test[\"num\"].values]+[data_test[\"cat\"].values],\n",
    "                                      data_test[\"labels\"].values),\n",
    "                                  steps_per_epoch=gen.get_no_batches(),\n",
    "                                  epochs=nb_epochs,\n",
    "                                  verbose=verbose,\n",
    "                                  max_q_size=10,\n",
    "                                  workers=1,\n",
    "                                  pickle_safe=True,class_weight=class_weights,\n",
    "                                  callbacks=[checkpoint, earlystopping])\n",
    "\n",
    "    else:\n",
    "        # fixed input and output shapes for generator\n",
    "        history = model.fit([data_train[\"num\"].values] +\n",
    "                            [data_train[\"cat\"].values],\n",
    "                            data_train[\"labels\"].values,\n",
    "                            validation_data=(\n",
    "                                [data_test[\"num\"].values]+[data_test[\"cat\"].values],\n",
    "                                      data_test[\"labels\"].values),\n",
    "                            shuffle=True,\n",
    "                            epochs=nb_epochs, batch_size=batch_size, class_weight=class_weights,\n",
    "                            verbose=verbose, callbacks=[checkpoint,earlystopping])\n",
    "\n",
    "\n",
    "    pickle.dump(history.history, open(\"f_history.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class_weights:  {0: 1.0, 1: 1.0}\n",
      "Oversampling with ration neg/pos= 1\n",
      "Epoch 1/10\n",
      "3623/3627 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8188 - precision: 0.8691 - recall: 0.7526 - fmeasure: 0.8060 - gmean: 0.8084Epoch 00000: val_acc improved from -inf to 0.75921, saving model to f_handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 55s - loss: 0.3945 - acc: 0.8188 - precision: 0.8691 - recall: 0.7526 - fmeasure: 0.8060 - gmean: 0.8084 - val_loss: 0.4086 - val_acc: 0.7592 - val_precision: 0.9983 - val_recall: 0.7593 - val_fmeasure: 0.8618 - val_gmean: 0.8702\n",
      "Epoch 2/10\n",
      "3624/3627 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8524 - precision: 0.9140 - recall: 0.7788 - fmeasure: 0.8406 - gmean: 0.8435Epoch 00001: val_acc improved from 0.75921 to 0.77683, saving model to f_handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 62s - loss: 0.3339 - acc: 0.8524 - precision: 0.9140 - recall: 0.7788 - fmeasure: 0.8406 - gmean: 0.8435 - val_loss: 0.3783 - val_acc: 0.7768 - val_precision: 0.9981 - val_recall: 0.7771 - val_fmeasure: 0.8732 - val_gmean: 0.8804\n",
      "Epoch 3/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8620 - precision: 0.9227 - recall: 0.7910 - fmeasure: 0.8513 - gmean: 0.8541Epoch 00002: val_acc improved from 0.77683 to 0.78845, saving model to f_handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 56s - loss: 0.3157 - acc: 0.8620 - precision: 0.9227 - recall: 0.7910 - fmeasure: 0.8513 - gmean: 0.8541 - val_loss: 0.3638 - val_acc: 0.7885 - val_precision: 0.9980 - val_recall: 0.7890 - val_fmeasure: 0.8806 - val_gmean: 0.8870\n",
      "Epoch 4/10\n",
      "3623/3627 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.8682 - precision: 0.9283 - recall: 0.7987 - fmeasure: 0.8582 - gmean: 0.8608Epoch 00003: val_acc improved from 0.78845 to 0.79856, saving model to f_handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 56s - loss: 0.3041 - acc: 0.8682 - precision: 0.9284 - recall: 0.7986 - fmeasure: 0.8582 - gmean: 0.8608 - val_loss: 0.3378 - val_acc: 0.7986 - val_precision: 0.9979 - val_recall: 0.7992 - val_fmeasure: 0.8870 - val_gmean: 0.8927\n",
      "Epoch 5/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.8730 - precision: 0.9321 - recall: 0.8054 - fmeasure: 0.8637 - gmean: 0.8662Epoch 00004: val_acc did not improve\n",
      "3627/3627 [==============================] - 53s - loss: 0.2951 - acc: 0.8730 - precision: 0.9321 - recall: 0.8054 - fmeasure: 0.8637 - gmean: 0.8662 - val_loss: 0.3390 - val_acc: 0.7958 - val_precision: 0.9979 - val_recall: 0.7965 - val_fmeasure: 0.8853 - val_gmean: 0.8912\n",
      "Epoch 6/10\n",
      "3623/3627 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.8764 - precision: 0.9349 - recall: 0.8098 - fmeasure: 0.8674 - gmean: 0.8699Epoch 00005: val_acc improved from 0.79856 to 0.80874, saving model to f_handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 56s - loss: 0.2887 - acc: 0.8764 - precision: 0.9349 - recall: 0.8098 - fmeasure: 0.8674 - gmean: 0.8699 - val_loss: 0.3329 - val_acc: 0.8087 - val_precision: 0.9979 - val_recall: 0.8095 - val_fmeasure: 0.8933 - val_gmean: 0.8985\n",
      "Epoch 7/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.8789 - precision: 0.9370 - recall: 0.8132 - fmeasure: 0.8702 - gmean: 0.8726Epoch 00006: val_acc did not improve\n",
      "3627/3627 [==============================] - 58s - loss: 0.2842 - acc: 0.8789 - precision: 0.9369 - recall: 0.8132 - fmeasure: 0.8702 - gmean: 0.8726 - val_loss: 0.3320 - val_acc: 0.8024 - val_precision: 0.9978 - val_recall: 0.8031 - val_fmeasure: 0.8894 - val_gmean: 0.8949\n",
      "Epoch 8/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.8819 - precision: 0.9392 - recall: 0.8173 - fmeasure: 0.8736 - gmean: 0.8759Epoch 00007: val_acc did not improve\n",
      "3627/3627 [==============================] - 61s - loss: 0.2793 - acc: 0.8819 - precision: 0.9393 - recall: 0.8173 - fmeasure: 0.8736 - gmean: 0.8759 - val_loss: 0.3447 - val_acc: 0.8018 - val_precision: 0.9979 - val_recall: 0.8025 - val_fmeasure: 0.8890 - val_gmean: 0.8946\n",
      "Epoch 9/10\n",
      "3625/3627 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.8835 - precision: 0.9400 - recall: 0.8201 - fmeasure: 0.8755 - gmean: 0.8778Epoch 00008: val_acc improved from 0.80874 to 0.82153, saving model to f_handset_weights.best.hdf5\n",
      "3627/3627 [==============================] - 65s - loss: 0.2757 - acc: 0.8835 - precision: 0.9400 - recall: 0.8202 - fmeasure: 0.8755 - gmean: 0.8778 - val_loss: 0.3045 - val_acc: 0.8215 - val_precision: 0.9976 - val_recall: 0.8226 - val_fmeasure: 0.9012 - val_gmean: 0.9056\n",
      "Epoch 10/10\n",
      "3626/3627 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.8853 - precision: 0.9414 - recall: 0.8225 - fmeasure: 0.8775 - gmean: 0.8797Epoch 00009: val_acc did not improve\n",
      "3627/3627 [==============================] - 66s - loss: 0.2716 - acc: 0.8853 - precision: 0.9414 - recall: 0.8225 - fmeasure: 0.8775 - gmean: 0.8797 - val_loss: 0.3226 - val_acc: 0.8158 - val_precision: 0.9977 - val_recall: 0.8168 - val_fmeasure: 0.8977 - val_gmean: 0.9025\n"
     ]
    }
   ],
   "source": [
    "chkp_file = \"f_handset_weights.best.hdf5\"\n",
    "\n",
    "# use local function\n",
    "train_and_evaluate_model(fmodel, fdata_train, fdata_test, nb_epochs=args.epochs,\n",
    "                         batch_size=args.batch_size,\n",
    "                         oversample=args.oversample,\n",
    "                         apply_class_weights=args.apply_class_weights,\n",
    "                         smooth_factor=args.smooth_factor,\n",
    "                         verbose=args.verbose, chkp_file=chkp_file,\n",
    "                         earlystop_pat=args.earlystop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_train = model.predict([data_train[\"num\"].values]+[data_train[\"cat\"].values])\n",
    "fp_train = fmodel.predict([fdata_train[\"num\"].values]+[fdata_train[\"cat\"].values])\n",
    "\n",
    "p_test = model.predict([data_test[\"num\"].values]+[data_test[\"cat\"].values])\n",
    "fp_test = fmodel.predict([fdata_test[\"num\"].values]+[fdata_test[\"cat\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "fpred_train = []\n",
    "for i in range(len(p_train)):\n",
    "    if fp_train[i] < threshold:\n",
    "        fpred_train.append(1)\n",
    "    else:\n",
    "        fpred_train.append(0)\n",
    "        \n",
    "fpred_test = []\n",
    "for i in range(len(p_test)):\n",
    "    if fp_test[i] < threshold:\n",
    "        fpred_test.append(1)\n",
    "    else:\n",
    "        fpred_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([95028, 21631]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fpred_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold2 = 0.5\n",
    "\n",
    "tpred_train = []\n",
    "for i in range(len(p_train)):\n",
    "    if p_train[i] > threshold2:\n",
    "        tpred_train.append(1)\n",
    "    else:\n",
    "        tpred_train.append(0)\n",
    "        \n",
    "tpred_test = []\n",
    "for i in range(len(p_test)):\n",
    "    if p_test[i] > threshold2:\n",
    "        tpred_test.append(1)\n",
    "    else:\n",
    "        tpred_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([95257, 21402]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tpred_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds = []\n",
    "for i in range(len(tpred_train)):\n",
    "    train_preds.append(tpred_train[i] & fpred_train[i])\n",
    "    \n",
    "test_preds = []\n",
    "for i in range(len(tpred_test)):\n",
    "    test_preds.append(tpred_test[i] & fpred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-auc: 0.919812\teval-auc: 0.708753\n",
      "train-accuracy: 0.848510\teval-accuracy: 0.844710\n",
      "train-precision: 0.031612\teval-precision: 0.018243\n",
      "train-recall: 0.991831\teval-recall: 0.571429\n",
      "train-confusion-matrix:\n",
      " [[393635  70671]\n",
      " [    19   2307]]\n",
      "test-confusion-matrix:\n",
      " [[98211 17867]\n",
      " [  249   332]]\n",
      "train-true-pos-rate: 0.991831\teval-true-pos-rate: 0.571429\n"
     ]
    }
   ],
   "source": [
    "y_train = data_train['labels']['TARGET_S_TO_S_APPLE']\n",
    "y_test = data_test['labels']['TARGET_S_TO_S_APPLE']\n",
    "\n",
    "roc_auc_train = roc_auc_score(y_train.values, train_preds)\n",
    "roc_auc_test = roc_auc_score(y_test.values, test_preds)\n",
    "\n",
    "a_train = accuracy_score(y_train.values, np.rint(train_preds))\n",
    "a_test = accuracy_score(y_test.values, np.rint(test_preds))\n",
    "\n",
    "prec_train = precision_score(y_train.values, np.rint(train_preds))\n",
    "prec_test = precision_score(y_test.values, np.rint(test_preds))\n",
    "\n",
    "r_train = recall_score(y_train.values, np.rint(train_preds))\n",
    "r_test = recall_score(y_test.values, np.rint(test_preds))\n",
    "\n",
    "m_train = confusion_matrix(y_train.values, np.rint(train_preds))\n",
    "m_test = confusion_matrix(y_test.values, np.rint(test_preds))\n",
    "true_pos_rate_train = m_train[1][1]/(m_train[1][1]+m_train[1][0])\n",
    "true_pos_rate_test = m_test[1][1]/(m_test[1][1]+m_test[1][0])\n",
    "\n",
    "print('train-auc: %f\\teval-auc: %f' % (roc_auc_train, roc_auc_test))\n",
    "print('train-accuracy: %f\\teval-accuracy: %f' % (a_train, a_test))\n",
    "print('train-precision: %f\\teval-precision: %f' % (prec_train, prec_test))\n",
    "print('train-recall: %f\\teval-recall: %f' % (r_train, r_test))\n",
    "\n",
    "print('train-confusion-matrix:\\n', m_train)\n",
    "print('test-confusion-matrix:\\n', m_test)\n",
    "print('train-true-pos-rate: %f\\teval-true-pos-rate: %f' % (true_pos_rate_train, true_pos_rate_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (handset-model)",
   "language": "python",
   "name": "handset-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
